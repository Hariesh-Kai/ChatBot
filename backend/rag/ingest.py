# backend/rag/ingest.py

import os
import json
from typing import List, Dict, Any

import psycopg2

from langchain_postgres import PGVector
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

# ============================================================
# GLOBAL CONFIG
# ============================================================

PROJECT_ROOT = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "../..")
)

HF_CACHE_DIR = os.path.join(PROJECT_ROOT, "models", "hf_cache")

COLLECTION_NAME = "rag_documents"

# ============================================================
# INTERNAL HELPERS
# ============================================================

def _normalize_conn(conn: str) -> str:
    if not conn:
        raise RuntimeError("DB connection string is required")
    return conn.replace("postgresql+psycopg2://", "postgresql://")

def _get_embeddings() -> HuggingFaceEmbeddings:
    # ðŸ”¥ CRITICAL FIX: Matches backend/api/chat.py
    # Using BGE-M3 allows the system to read massive tables without truncation.
    return HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        cache_folder=HF_CACHE_DIR,
        model_kwargs={"device": "cpu"}, # Set to "cuda" if you have a GPU
        encode_kwargs={"normalize_embeddings": True}
    )

def _get_vector_store(connection_string: str) -> PGVector:
    return PGVector.from_existing_index(
        embedding=_get_embeddings(),
        collection_name=COLLECTION_NAME,
        connection=connection_string,
    )

# ============================================================
# LOAD DOCUMENTS (FIXED: CAPTURE CHUNK ID)
# ============================================================

def load_documents(json_path: str) -> List[Document]:
    """
    Load enriched chunks from JSON into LangChain Documents.
    
    ðŸ”¥ FIX 1: Flattens cmetadata into main metadata.
    ðŸ”¥ FIX 2: Explicitly captures 'chunk_id' from top-level JSON.
    """

    with open(json_path, "r", encoding="utf-8") as f:
        raw = json.load(f)

    documents: List[Document] = []

    for item in raw:
        content = item.get("page_content")
        metadata = item.get("metadata", {})
        cmetadata = item.get("cmetadata")
        
        # âœ… NEW: Capture the ID generated by metadata.py
        chunk_id = item.get("chunk_id")

        if not content or not cmetadata:
            continue

        if "company_document_id" not in cmetadata:
            raise RuntimeError("Missing company_document_id in cmetadata")

        if "revision_number" not in cmetadata:
            raise RuntimeError("Missing revision_number in cmetadata")

        # ðŸ”¥ FLATTEN: Merge identity directly into top-level metadata
        combined_metadata = {
            **metadata,
            **cmetadata 
        }
        
        # âœ… NEW: Inject chunk_id into metadata so it gets saved to DB
        if chunk_id:
            combined_metadata["chunk_id"] = chunk_id

        documents.append(
            Document(
                page_content=content,
                metadata=combined_metadata,
            )
        )

    if not documents:
        raise RuntimeError("No valid chunks loaded from JSON")

    return documents


# ============================================================
# INGEST DOCUMENT REVISION (NO DELETION, REVISION SAFE)
# ============================================================

def ingest_to_pgvector(
    *,
    documents: List[Document],
    connection_string: str,
    company_document_id: str,
    revision_number: str, # âœ… FIX: Changed to str for enterprise support
) -> None:
    """
    Ingest a document revision into PGVector.
    """

    if not documents:
        raise RuntimeError("No documents provided for ingestion")

    vector_store = _get_vector_store(connection_string)

    # --------------------------------------------------------
    # ðŸ”’ DEFENSIVE IDENTITY CHECK (FIXED)
    # --------------------------------------------------------

    for doc in documents:
        # Check top-level metadata (since we flattened it)
        cm = doc.metadata 

        if cm.get("company_document_id") != company_document_id:
            raise RuntimeError(
                "company_document_id mismatch during ingest"
            )

        # âœ… FIX: Strict string comparison for revisions
        if str(cm.get("revision_number")) != str(revision_number):
            raise RuntimeError(
                f"revision_number mismatch during ingest: "
                f"doc={cm.get('revision_number')} expected={revision_number}"
            )

    # --------------------------------------------------------
    # INGEST
    # --------------------------------------------------------

    vector_store.add_documents(documents)

    setup_keyword_search(connection_string)


# ============================================================
# METADATA UPDATE (POST-CONFIRMATION)
# ============================================================

def update_vector_metadata(
    *,
    connection_string: str,
    company_document_id: str,
    revision_number: str, 
    updated_metadata: Dict[str, Any],
) -> None:
    """
    Update cmetadata for ALL chunks of a document revision.
    """

    if not updated_metadata:
        return

    conn = psycopg2.connect(_normalize_conn(connection_string))
    cur = conn.cursor()

    cur.execute(
        """
        UPDATE langchain_pg_embedding
        SET cmetadata = cmetadata || %s::jsonb
        WHERE cmetadata->>'company_document_id' = %s
          AND cmetadata->>'revision_number' = %s
        """,
        (
            json.dumps(updated_metadata),
            company_document_id,
            str(revision_number),
        ),
    )

    conn.commit()
    cur.close()
    conn.close()


# ============================================================
# DUPLICATE METADATA CHECK
# ============================================================

def metadata_exists(
    *,
    connection_string: str,
    metadata: Dict[str, Any],
) -> bool:
    """
    Check if similar cmetadata already exists.
    """

    if not metadata:
        return False

    conn = psycopg2.connect(_normalize_conn(connection_string))
    cur = conn.cursor()

    conditions = []
    values = []

    for k, v in metadata.items():
        conditions.append("cmetadata->>%s = %s")
        values.extend([k, str(v)])

    query = f"""
        SELECT 1
        FROM langchain_pg_embedding
        WHERE {' AND '.join(conditions)}
        LIMIT 1
    """

    cur.execute(query, values)
    exists = cur.fetchone() is not None

    cur.close()
    conn.close()

    return exists


# ============================================================
# KEYWORD SEARCH SUPPORT
# ============================================================

def setup_keyword_search(connection_string: str) -> None:
    """
    Create keyword search index (idempotent).
    """

    conn = psycopg2.connect(_normalize_conn(connection_string))
    cur = conn.cursor()

    cur.execute(
        """
        ALTER TABLE langchain_pg_embedding
        ADD COLUMN IF NOT EXISTS content_tsv tsvector
        GENERATED ALWAYS AS (
            to_tsvector('english', document)
        ) STORED;
        """
    )

    cur.execute(
        """
        CREATE INDEX IF NOT EXISTS
        langchain_pg_embedding_content_tsv_idx
        ON langchain_pg_embedding
        USING GIN (content_tsv);
        """
    )

    conn.commit()
    cur.close()
    conn.close()